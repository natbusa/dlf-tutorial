{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalabframework\n",
    "\n",
    "The datalabframework is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SparkEngine\n",
      "Init engine \"spark\"\n",
      "Configuring packages:\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "Configuring conf:\n",
      "  -  spark.hadoop.fs.s3a.access.key : ****** (redacted)\n",
      "  -  spark.hadoop.fs.s3a.endpoint : http://minio:9000\n",
      "  -  spark.hadoop.fs.s3a.impl : org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "  -  spark.hadoop.fs.s3a.path.style.access : true\n",
      "  -  spark.hadoop.fs.s3a.secret.key : ****** (redacted)\n",
      "Connecting to spark master: local[*]\n",
      "Engine context spark:2.4.1 successfully started\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datalabframework.project.Project at 0x7f281edd7668>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal(a,b):\n",
    "    cnt = a.exceptAll(b).count() + b.exceptAll(a).count()\n",
    "    return cnt==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local files\n",
    "\n",
    "The following show some load/save round trip on the local file system using various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|  a|  b|   c|\n",
      "+---+---+----+\n",
      "|yes|  1|1.41|\n",
      "| no|  0|3.14|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "df = dlf.engine().load('data/examples/sample.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save in various format\n",
    "dlf.save(df, 'data/save/foo.csv')\n",
    "dlf.save(df, 'data/save/foo.json')\n",
    "dlf.save(df, 'data/save/foo.parquet')\n",
    "\n",
    "# with various compression\n",
    "dlf.save(df, 'data/save/foo.json.bz2')\n",
    "dlf.save(df, 'data/save/foo.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using format specific save\n",
    "\n",
    "dlf.save_csv(df, 'data/save/bar.csv')\n",
    "dlf.save_json(df, 'data/save/bar.json')\n",
    "dlf.save_parquet(df, 'data/save/bar.parquet')\n",
    "\n",
    "# with various compression\n",
    "dlf.save_json(df, 'data/save/bar.json.bz2')\n",
    "dlf.save_csv(df, 'data/save/bar.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round trip reading\n",
    "df_tst = dlf.load('data/save/foo.csv')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/save/foo.json')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/save/foo.parquet')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/save/foo.json.bz2')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/save/foo.csv.gz')\n",
    "assert(equal(df,df_tst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round trip reading (format specific load)\n",
    "df_tst = dlf.load_csv('data/save/bar.csv')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load_json('data/save/bar.json')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load_parquet('data/save/bar.parquet')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load_json('data/save/bar.json.bz2')\n",
    "#assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load_csv('data/save/bar.csv.gz')\n",
    "assert(equal(df,df_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data from HDFS\n",
    "\n",
    "We can override the default resource provider, \n",
    "by explicitely passing a different provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.save(df, 'data/examples/bar.csv', 'hdfs')\n",
    "dlf.save(df, 'data/examples/bar.json', 'hdfs')\n",
    "dlf.save(df, 'data/examples/bar.parquet', 'hdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst = dlf.load('data/examples/bar.csv', 'hdfs')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/examples/bar.json', 'hdfs')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/examples/bar.parquet', 'hdfs')\n",
    "assert(equal(df,df_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data from Minio\n",
    "\n",
    "We can override the default resource provider, \n",
    "by explicitely passing a different provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.save(df, 'data/examples/bar.csv', 'minio')\n",
    "dlf.save(df, 'data/examples/bar.json', 'minio')\n",
    "dlf.save(df, 'data/examples/bar.parquet', 'minio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tst = dlf.load('data/examples/bar.csv', 'minio')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/examples/bar.json', 'minio')\n",
    "assert(equal(df,df_tst))\n",
    "\n",
    "df_tst = dlf.load('data/examples/bar.parquet', 'minio')\n",
    "assert(equal(df,df_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access data from a jdbc connection\n",
    "\n",
    "We can override the default resource provider, \n",
    "by explicitely passing a different provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MySQL: Sakila DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|last_name|amount|\n",
      "+---------+------+\n",
      "|    ABNEY|    21|\n",
      "|     ADAM|    28|\n",
      "|    ADAMS|    27|\n",
      "|ALEXANDER|    27|\n",
      "|   ALLARD|    32|\n",
      "|    ALLEN|    31|\n",
      "|  ALVAREZ|    27|\n",
      "| ANDERSON|    24|\n",
      "|   ANDREW|    25|\n",
      "|  ANDREWS|    23|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT c.last_name,\n",
    "        COUNT(p.amount) AS amount\n",
    "    FROM customer c\n",
    "    LEFT JOIN payment p\n",
    "        ON c.customer_id = p.customer_id\n",
    "    WHERE c.last_name like 'A%'\n",
    "    GROUP BY  c.last_name\n",
    "    ORDER BY  c.last_name ASC\n",
    "    LIMIT 10;\n",
    "\"\"\"\n",
    "dlf.load(query, 'sakila').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------+\n",
      "|customer_id|store_id|first_name|last_name|\n",
      "+-----------+--------+----------+---------+\n",
      "|          1|       1|      MARY|    SMITH|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|\n",
      "|          3|       1|     LINDA| WILLIAMS|\n",
      "|          4|       2|   BARBARA|    JONES|\n",
      "|          5|       1| ELIZABETH|    BROWN|\n",
      "|          6|       2|  JENNIFER|    DAVIS|\n",
      "|          7|       1|     MARIA|   MILLER|\n",
      "|          8|       2|     SUSAN|   WILSON|\n",
      "|          9|       2|  MARGARET|    MOORE|\n",
      "|         10|       1|   DOROTHY|   TAYLOR|\n",
      "+-----------+--------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = dlf.load('customer', 'sakila')\n",
    "df.select('customer_id', 'store_id', 'first_name', 'last_name').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postgres: Pagila DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SparkEngine\n",
      "Init engine \"spark\"\n",
      "Configuring packages:\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "Configuring conf:\n",
      "  -  spark.hadoop.fs.s3a.access.key : ****** (redacted)\n",
      "  -  spark.hadoop.fs.s3a.endpoint : http://minio:9000\n",
      "  -  spark.hadoop.fs.s3a.impl : org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "  -  spark.hadoop.fs.s3a.path.style.access : true\n",
      "  -  spark.hadoop.fs.s3a.secret.key : ****** (redacted)\n",
      "Connecting to spark master: local[*]\n",
      "Engine context spark:2.4.1 successfully started\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datalabframework.project.Project at 0x7fcf173acf98>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|16049|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query from resource\n",
    "md = dlf.Resource('select count(*) from payment', 'pagila')\n",
    "dlf.load(md).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|total_sales|last_name|first_name|\n",
      "+-----------+---------+----------+\n",
      "|   33489.47|  Hillyer|      Mike|\n",
      "|   33927.04| Stephens|       Jon|\n",
      "+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use JOIN to display the total amount rung up by each staff member\n",
    "# use tables 'staff' and 'payment'\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        CAST(SUM(p.amount) AS DECIMAL(16,2)) as total_sales, \n",
    "        s.last_name, \n",
    "        s.first_name\n",
    "    FROM payment p \n",
    "    INNER JOIN staff s ON p.staff_id = s.staff_id \n",
    "    GROUP BY s.last_name, s.first_name\n",
    "    \"\"\"\n",
    "df = dlf.load(query, 'pagila')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = dlf.Resource('total_sales', 'pagila')\n",
    "dlf.save(df, md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|total_sales|last_name|first_name|\n",
      "+-----------+---------+----------+\n",
      "|   33489.47|  Hillyer|      Mike|\n",
      "|   33927.04| Stephens|       Jon|\n",
      "+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# round trip read back\n",
    "dlf.load('total_sales', 'pagila').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalabframework 0.8.1\n"
     ]
    }
   ],
   "source": [
    "# check if the pyspark DataFrame class is monkey patched\n",
    "df.datalabframework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|total_sales|last_name|first_name|\n",
      "+-----------+---------+----------+\n",
      "|   12345.67|   Dereck|       Eve|\n",
      "+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate a new dataframe from the original one, \n",
    "# by providing new data and retaining the original schema\n",
    "\n",
    "from decimal import Decimal as d\n",
    "df = df.rows.overwrite([(d(12345.67),'Dereck', 'Eve')])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|total_sales|last_name|first_name|\n",
      "+-----------+---------+----------+\n",
      "|   33489.47|  Hillyer|      Mike|\n",
      "|   33927.04| Stephens|       Jon|\n",
      "|   12345.67|   Dereck|       Eve|\n",
      "+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# append new records and reload\n",
    "dlf.save(df, 'total_sales', 'pagila', mode='append')\n",
    "dlf.load('total_sales', 'pagila').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From one provider to the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.load('payment', 'pagila').save('data/payment', 'hdfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a better grid :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qgrid\n",
    "qgrid.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a1eab67e4a4cac804b7b507d3dbec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defauâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = dlf.load('data/payment', 'hdfs').limit(10)\n",
    "qgrid.show_grid(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
